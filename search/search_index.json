{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HPMPC: High Performance Secure Multi-Party Computation","text":"<p>HPMPC is a C++ framework for Secure Multi-Party Computation (MPC) that provides various tweaks and design choices to scale MPC programs to large-scale workloads. The framework contains the following software components.</p> Component Description Dependencies <code>Core</code> Implements networking, cryptographic primitives, and hardware acceleration techniques such as Bitslicing, Vectorization, and GPU acceleration. - <code>Protocols</code> Implements various MPC protocols with a fixed interface for each primitive. Utilizes high-level interfaces from <code>Core</code> for networking and cryptographic primitives. <code>Datatypes</code> Provides datatypes for arithmetic circuits, boolean circuits, and fixed-point arithmetic. Maps each operation on secret shares to the primitives implemented by a <code>Protocol</code>. <code>Programs</code> Implements high-level functions, routines, use cases, tests, and benchmarks. Utilizes <code>Datatypes</code> to offer a generic programming interface for MPC. <code>Measurements</code> Provides config files and scripts to automate benchmarking and testing of MPC workloads. Uses implemented <code>Programs</code> to run benchmarks and tests. <code>Neural Networks</code> Implements neural network inference with MPC. Combines functions implemented in <code>Programs</code> with <code>PIGEON</code>, a templated neural network inference engine to perform private inference <p>The framework is designed to be modular and extensible, allowing users to implement new MPC protocols and functions without in-depth knowledge of other components.</p> <p>Tip</p> <p>Click on the components to navigate to their respective detailed documentation sites.  HPMPC also provides an extensive set of configuration options to customize MPC programs, optimize their performances, and support various hardware architectures. For more details, refer to Configurations.</p>"},{"location":"#protocols-overview","title":"Protocols (Overview)","text":"<p>Out of the box, the framework provides multiple MPC protocols that support different primitives. The basic primitives cover secret sharing revealing, addition, and multiplication, which enables computing functions such as AES that do not require share conversion. Protocols supporting all primitives can also evaluate mixed circuits and fixed-point arithmetic. </p> <p>Protocols can be selected with the <code>PROTOCOL</code> configuration option when compiling an executable. For instance, setting <code>PROTOCOL=5</code> will compile the executable with Protocol 5 (Trio).  The preprocessing phases allow shifting some computation and communication to an input-independent phase before the online phase. This phase is optional and can be activated using the <code>PRE=1</code> config option when compiling an executable. Maliciously secure protocols print a terminal message when a hash-based consistency check fails to indicate aborting the computation.</p> Protocol Adversary Model Preprocessing Supported Primitives <code>1</code> Sharemind (3PC) Semi-Honest \u2718 Basic <code>2</code> Replicated (3PC) Semi-Honest \u2718 Basic <code>3</code> ASTRA (3PC) Semi-Honest \u2714 Basic <code>4</code> ABY2 Dummy (2PC) Semi-Honest \u2714 Basic <code>5</code> Trio (3PC) Semi-Honest \u2714 All <code>6</code> Trusted Third Party (3PC) Semi-Honest \u2718 All <code>7</code> Trusted Third Party (4PC) Semi-Honest \u2718 All <code>8</code> Tetrad (4PC) Malicious \u2714 Basic <code>9</code> Fantastic Four (4PC) Malicious \u2718 Basic <code>10</code> Quad (4PC) Malicious \u2718 All <code>11</code> Quad: Het (4PC) Malicious \u2714 All <code>12</code> Quad (4PC) Malicious \u2714 All <p>Tip</p> <p>For most applications <code>PROTOCOL=5</code> and <code>PROTOCOL=12</code> are recommended.</p>"},{"location":"#programs-overview","title":"Programs (Overview)","text":"<p>Each predefined program is mapped to a <code>FUNCTION_IDENTIFIER</code> that can be set when compiling an executable.  The <code>protocol_executer.hpp</code> file contains the mapping of <code>FUNCTION_IDENTIFIER</code> to their corresponding source files. For instance, <code>FUNCTION_IDENTIFIER</code> 0-7 are reserved for benchmarking different basic primitives.  Programs can import <code>Datatypes</code> and existing <code>Functions</code> such as comparisons to program MPC workloads.</p> <p>Note</p> <p>There are multiple tutorials in <code>programs/tutorial</code> that demonstrate how to implement MPC programs using the framework's interface.</p>"},{"location":"#configurations-overview","title":"Configurations (Overview)","text":"<p><code>Programs</code> can be compiled with various configurations. For instance, a program might use a certain <code>BITLENGTH</code>, <code>PROTOCOL</code>, and number of <code>FRACTIONAL</code> bits for fixed-point arithmetic. These config options are fetched from the <code>config.h</code> file which contains extensive configurations for details and tweaks to run MPC programs. Changes can be made permanently in the <code>config.h</code> file or temporarily by specifying the option as an argument to the <code>make</code> command. <pre><code>make -j PARTY=&lt;party_id&gt; FUNCTION_IDENTIFIER=&lt;function_identifier&gt; BITLENGTH=&lt;bitlength&gt;\n</code></pre></p>"},{"location":"#the-vectorized-programming-model-overview","title":"The Vectorized Programming Model (Overview)","text":"<p>The framework uses the register length specified by the <code>DATTYPE</code> config option for all secret shares. This means that each arithmetic secret share is inherently a vector of <code>DATTYPE/BITLENGTH</code> elements and each boolean secret share is a vector of <code>DATTYPE</code> elements. When setting <code>NUM_PROCESSES</code> to a value greater than 1, the framework will additionally execute the same program on multiple processes in parallel.  Applications such as neural network inference, or AES computations especially benefit from this parallelization approach as they require operations on independent batches of data. Additionally, the <code>SPLITROLES</code> config option can be used to execute the same program for all party permutations in parallel to achieve load balancing across parties.</p> <p>To turn off any kind of inherent parallelization, set <code>DATTYPE=BITLENGTH</code> (or <code>DATTYPE=1</code> for boolean circuits), <code>NUM_PROCESSES=1</code>, and <code>SPLITROLES=0</code>. This will result in a single-threaded execution of the program without any vectorization.</p> <p>Warning</p> <p>Not all <code>DATTYPE</code> values are supported by all hardware architectures. Additionally, some <code>BITLENGTH</code>s are not supported by certain DATTYPEs. Learn more here.  Understanding the vectorized programming model is crucial to writing correct programs. Refer to the tutorials and detailed documentation for more information.</p>"},{"location":"#compile-and-run-programs","title":"Compile and Run Programs","text":"<p>You can use the provided Dockerfile or set up the project manually. The only dependency is OpenSSL. Neural Networks and other functions with matrix operations also require the Eigen library. Install on your target system, for instance via <code>apt install libssl-dev libeigen3-dev</code>. </p> <p>To use GPU acceleration for matrix multiplication and convolutions, you need an NVIDIA GPU and the NVCC compiler. You also need a copy of the CUTLASS library. You can then set up the project as follows.</p>"},{"location":"#setup","title":"Setup","text":"<pre><code># General dependencies\nsudo apt install libssl-dev libeigen3-dev\n\n# Dependencies for GPU acceleration (optional)\ngit clone https://github.com/NVIDIA/cutlass.git\n</code></pre>"},{"location":"#basic-setting","title":"Basic Setting","text":"<p>Programs can be compiled and executed for a specific party in distributed settings or for all parties locally.</p> <pre><code># Compile and run for all parties\nmake -j PARTY=all FUNCTION_IDENTIFIER=&lt;function_identifier&gt; PROTOCOL=&lt;protocol_id&gt;\nscripts/run.sh -p all -n &lt;num_parties&gt;\n</code></pre> <pre><code># Compile and run for P0, repeat for other parties analogously\nmake -j PARTY=0 FUNCTION_IDENTIFIER=&lt;function_identifier&gt; PROTOCOL=&lt;protocol_id&gt;\nscripts/run.sh -p 0 -a &lt;ip_address_p0&gt; -b &lt;ip_address_p1&gt; -c &lt;ip_address_p2&gt; -d &lt;ip_address_p3&gt;\n</code></pre>"},{"location":"#splitroles","title":"SplitRoles","text":"<p>For additional load balancing, <code>SPLITROLES</code> can be set to 1 to execute a 3PC program for all 6 party permutations in parallel, 2 to execute a 3PC program on four nodes for all 24 party permutations, and 3 to execute a 4PC program on four nodes for all 24 party permutations.</p> <pre><code># Compile and run for all parties\nmake -j PARTY=all FUNCTION_IDENTIFIER=&lt;function_identifier&gt; PROTOCOL=&lt;protocol_id&gt; SPLITROLES=&lt;splitroles_id&gt;\nscripts/run.sh -p all -n &lt;num_parties&gt; -s &lt;splitroles_id&gt;\n</code></pre> <pre><code># Compile and run for P0, repeat for other parties analogously\nmake -j PARTY=0 FUNCTION_IDENTIFIER=&lt;function_identifier&gt; PROTOCOL=&lt;protocol_id&gt; SPLITROLES=&lt;splitroles_id&gt;\nscripts/run.sh -p 0 -a &lt;ip_address_p0&gt; -b &lt;ip_address_p1&gt; -c &lt;ip_address_p2&gt; -d &lt;ip_address_p3&gt; -s &lt;splitroles_id&gt;\n</code></pre>"},{"location":"#gpu-acceleration","title":"GPU Acceleration","text":"<p>The framework supports GPU acceleration for matrix multiplication by using CUDA. This requires to first compile the standalone CUDA executables.</p> <pre><code># Cutlass is required for GPU acceleration\ngit clone https://github.com/NVIDIA/cutlass.git\ncd core/cuda\n# Replace with your architecture, nvcc path, and CUTLASS path:\nmake -j arch=sm_89 CUDA_PATH=/usr/local/cuda CUTLASS_PATH=/home/user/cutlass \n</code></pre> <p>The framework implements different approaches to accelerate convolutions and matrix multiplications <code>USE_CUDA_GEMM=2</code> or <code>USE_CUDA_GEMM=4</code> are good starting points for most applications. <pre><code># Compile and run for all parties\nmake -j PARTY=all FUNCTION_IDENTIFIER=&lt;function_identifier&gt; PROTOCOL=&lt;protocol_id&gt; USE_CUDA_GEMM=2```\n</code></pre></p> <pre><code># Compile and run for P0, repeat for other parties analogously\nmake -j PARTY=0 FUNCTION_IDENTIFIER=&lt;function_identifier&gt; PROTOCOL=&lt;protocol_id&gt; USE_CUDA_GEMM=2```\n</code></pre>"},{"location":"#multi-gpu-acceleration","title":"Multi-GPU Acceleration","text":"<p>If each node has access to multiple GPUs these can be utilized with the <code>-g</code> option at runtime. <pre><code># Run SplitRoles executable for P0 and utilize 6 GPUs for convolutions and matrix multiplications\nscripts/run.sh -p 0 -a &lt;ip_address_p0&gt; -b &lt;ip_address_p1&gt; -c &lt;ip_address_p2&gt; -d &lt;ip_address_p3&gt; -s 1 -g 6\n</code></pre></p>"},{"location":"#neural-networks-overview","title":"Neural Networks (Overview)","text":"<p>The <code>PIGEON</code> (Private Inference of Neural Networks) submodule is a templated neural network inference engine that can be used to run neural networks using MPC.  The second submodule, <code>PyGEON</code>, enables training and exporting models and datasets in PyTorch and importing them to <code>PIGEON</code> for inference. The <code>NN</code> program orchestrates the execution of <code>PIGEON</code> and the MPC backend. </p>"},{"location":"#setup_1","title":"Setup","text":"<pre><code># Load Submodules for Neural Networks\ngit submodule update --init --recursive\n\n# Dependencies for training and exporting models and datasets\npip install torch torchvision \n\n# Dependencies for downloading pretrained models and datasets\npip install gdown \n</code></pre>"},{"location":"#run-private-inference","title":"Run Private Inference","text":"<p>The <code>programs/NN.hpp</code> file contains a mapping of various neural network architectures to a <code>FUNCTION_IDENTIFIER</code>.  The following command compiles a neural network inference of <code>VGG16</code> on the CIFAR-10 dataset (<code>FUNCTION_IDENTIFIER=74</code>).  The batch size in the following example is <code>NUM_INPUTS</code> \\(\\cdot\\) <code>DATTYPE</code>/<code>BITLENGTH</code> \\(\\cdot\\) <code>PROCESS_NUM</code> \\(\\cdot\\) <code>SPLITROLES_FACTOR</code> \\(=\\) 1 \\(\\cdot\\) 256/32 \\(\\cdot\\) 4 \\(\\cdot\\) 6 \\(=\\) 192.</p> <pre><code>`SPLITROLES_FACTOR \nmake -j PARTY=&lt;party_id&gt; FUNCTION_IDENTIFIER=74 BITLENGTH=32 FRACTIONAL=5 DATTYPE=256 SPLITROLES=1 PROCESS_NUM=4 PROTOCOL=5 NUM_INPUTS=1 USE_CUDA_GEMM=2\n</code></pre> <p>The command above uses dummy weights and datasets. To use real weights and datasets, the <code>MODELOWNER</code> and <code>DATAOWNER</code> config options must be set to the party that holds the model and the party that holds the dataset respectively. Additionally, pretrained models need to be available for importing.</p> <pre><code>cd nn/Pygeon\n# Option 1: Train a VGG16 model and export it to PyGEON \n python main.py --action train --export_model --export_dataset --transform standard --model VGG16 --num_classes 10 --dataset_name CIFAR-10 --modelpath ./models/alexnet_cifar --num_epochs 30 --lr 0.01 --criterion CrossEntropyLoss --optimizer Adam\n# Option 2: Download a pretrained VGG16 model and CIFAR10 dataset\npython download_pretrained.py single_model datasets\n\n# Set environment variables for the party holding the model parameters\nexport MODEL_DIR=nn/Pygeon/models/pretrained\nexport MODEL_FILE=VGG16_CIFAR-10_standard.bin #adjust path if needed\n\n# Set environment variables for the party holding the dataset\nexport DATA_DIR=nn/Pygeon/data/datasets\nexport SAMPLES_FILE=CIFAR-10_standard_test_images.bin\nexport LABELS_FILE=CIFAR-10_standard_test_labels.bin #adjust path if needed\n\n#Compile the program, specify the party that holds the model and the dataset\nmake -j PARTY=&lt;party_id&gt; MODELOWNER=P_0 DATAOWNER=P_1 FUNCTION_IDENTIFIER=74 \n\n# Run the program\nscripts/run.sh -p &lt;party_id&gt; -a &lt;ip_address_a&gt; -b &lt;ip_address_b&gt; -c &lt;ip_address_c&gt; -d &lt;ip_address_d&gt; \n</code></pre> <p>Tip</p> <p>There is an extensive set of configuration options to optimize the performance of Neural Network inference such as different truncation and adder approaches. For more details, refer to Neural Network Settings.</p>"},{"location":"#measurements-overview","title":"Measurements (Overview)","text":"<p>To automate benchmarking of multiple configurations, the framework provides configuration files in <code>measurements/configs/</code>. By using the associated <code>measurements/run_config.py</code> and <code>measurements/parse_logs.py</code> scripts, programs can be compiled and executed with various configurations while results are stored in the <code>measurements/logs/</code> directory.  The following command is an example of running a benchmark and parsing the results. <pre><code>./measurements/run_config.py -p &lt;party_id&gt; -a &lt;ip_address_a&gt; -b &lt;ip_address_b&gt; -c &lt;ip_address_c&gt; -d &lt;ip_address_d&gt; -i 10 measurements/configs/&lt;config_file&gt;.conf \n./measurements/parse_logs.py measurements/logs/&lt;log_file&gt;.log\n</code></pre></p> <p>Note</p> <p>The --override option allows specifying a different value than the one in the config file. This might be necessary to adjust a <code>.conf</code> file to a specific target hardware. Learn more here.</p>"},{"location":"configurations/","title":"Configurations","text":"<p>HPMPC provides extensive configuration options to customize MPC programs, optimize their performances, and support various hardware architectures. Understanding the configuration options is crucial to achieving the best performance for a specific use case. The following sections provide an overview of the most important configuration options and tips on how to set them. A complete list of configuration options can be found in the <code>config.h</code> file. By default, the <code>config.h</code> file contains tweaks that should work well for most use cases.</p> <p>To change a configuration option, specify the option in the Makefile for temporary changes or in the <code>config.h</code> file for permanent changes that also apply to all future compilations. By default, the Makefile sets the configuration options to the ones in <code>config.h</code>. Every configuration option can be overridden by the Makefile by specifying the option as an argument to the <code>make</code> command.  <pre><code>make -j PARTY=&lt;party_id&gt; FUNCTION_IDENTIFIER=&lt;function_identifier&gt; NUM_INPUTS=&lt;num_inputs&gt;\n</code></pre></p>"},{"location":"configurations/#basic-settings","title":"Basic Settings","text":"Configuration Option Description Tips <code>PROTOCOL</code> Protocol to use for the MPC computation. Protocol 5 (Trio) is recommended for 3PC, Protocol 12 (Quad) for 4PC. <code>PRE</code> Use a preprocessing phase? Preprocessing is supported by Protocols 3, 5, 8, 11, 12. <code>PARTY</code> Party ID (starting from 0). For local execution, <code>PARTY=all</code> can be set by the <code>Makefile</code>. <code>BITLENGTH</code> Bitlength of integers. Lower BITLENGTHs can improve performance. <code>FRACTIONAL</code> Fractional bits to use for fixed-point arithmetic. Higher values can improve precision but perform worse with probabilistic truncation. <code>FUNCTION_IDENTIFIER</code> Identifier of the function to run. Refer to <code>protocol_executer.hpp</code> for a list of different predefined functions. <code>NUM_INPUTS</code> Number of inputs. Used by benchmarking functions or neural networks to set the number of inputs processed (multiplies with concurrency settings and <code>`SPLITROLES</code>)."},{"location":"configurations/#concurrency-settings","title":"Concurrency Settings","text":"Configuration Option Description Tips <code>DATTYPE</code> Register size to use for SIMD parallelization. Use the maximum supported by the hardware (e.g. 512 if the CPU supports AVX-512). Note that some BITLENGTHs are not supported by all DATTYPEs. <code>PROCESS_NUM</code> Number of parallel processes to use. Experiment with different values to find the optimal performance."},{"location":"configurations/#hardware-acceleration-settings","title":"Hardware Acceleration Settings","text":"Configuration Option Description Tips <code>RANDOM_ALGORITHM</code> Random number generation algorithm. 2 uses VAES or AES-NI, and 1 uses Bitslicing. Prefer 2 for performance. <code>USE_SSL_AES</code> Use OPENSSL's AES implementation. Prefer 0 for performance. <code>ARM</code> ARM processor support. Set to 1 for ARM processors, 0 otherwise. <code>USE_CUDA_GEMM</code> Use CUDA for matrix multiplication. Prefer 2 or 4 for performance."},{"location":"configurations/#tweaks","title":"Tweaks","text":"Configuration Option Description Tips <code>SEND_BUFFER</code> Number of messages to buffer before sending. Experiment with different values to find the optimal performance. <code>RECV_BUFFER</code> Number of receiving messages to buffer. Experiment with different values to find the optimal performance. <code>VERIFY_BUFFER</code> Number of messages to buffer before hashing. The lower the better, but must be at least 512/<code>DATTYPE</code> for correctness."},{"location":"configurations/#neural-network-settings","title":"Neural Network Settings","text":"Configuration Option Description Tips <code>MODELOWNER</code> Who holds the model parameters? Use <code>P_0</code> for party 0, <code>P_1</code> for party 1, etc. Use -1 to use dummy model parameters. <code>DATAOWNER</code> Who holds the data? Use <code>P_0</code> for party 0, <code>P_1</code> for party 1, etc. Use -1 to use dummy data. <code>TRUNC_THEN_MULT</code> Truncate before or after multiplication. Set to 1 for higher accuracy in most cases. <code>TRUNC_APPROACH</code> Truncation approach. Use <code>0</code> for stochastic truncation with a large slack; <code>1</code> for stochastic truncation with a slack of 1; <code>2</code> for exact truncation with a slack of 1; <code>3</code> for exact truncation with slack of 0; <code>4</code> for mixed stochastic truncation Prefer <code>0</code> for performance and <code>1</code>, <code>2</code>, or <code>4</code> for accuracy. <code>TRUNC_DELAYED</code> Delay CONV truncation until next ReLU. Must be set to 1 for truncation approaches 1 and 2. <code>COMPUTE_ARGMAX</code> Compute final Argmax during inference. Set to <code>0</code> to reveal class probabilities. <code>PUBLIC_WEIGHTS</code> Use public weights. Set to <code>1</code> to use public weights for improved performance. <code>COMPRESS</code> Use 8-bit ReLUs for all layers. Set to <code>1</code> to benchmark ReLUs with reduced bitlength. <code>REDUCED_BITLENGTH_k</code> cut off BITLENGTH-k least significant bits before sign bit extraction. May reduce accuracy significantly. <code>REDUCED_BITLENGTH_m</code> cut off m most significant bits before sign bit extraction. May reduce accuracy significantly. <code>IS_TRAINING</code> Training or inference phase. Training is not supported yet. <p>For different adder approaches there are also the <code>ONLINE_OPTIMIZED</code> and <code>BANDWIDTH_OPTIMIZED</code> options, where</p> <ul> <li><code>BANDWIDTH_OPTIMIZED=1</code> uses a Ripple Carry Adder (RCA) approach.</li> <li><code>BANDWIDTH_OPTIMIZED=0</code> and <code>ONLINE_OPTIMIZED=0</code> uses a Parallel Prefix Adder (PPA) approach.</li> <li><code>BANDWIDTH_OPTIMIZED=0</code> and <code>ONLINE_OPTIMIZED=1</code> uses a 4-Way parallel prefix adder approach (PPA4).</li> </ul> <p>For the predefined neural networks a <code>FUNCTION_IDENTIFIER&lt;100</code> uses RCA, <code>FUNCTION_IDENTIFIER&lt;200</code> uses PPA, and <code>FUNCTION_IDENTIFIER&lt;300</code> uses PPA4. For instance <code>FUNCTION_IDENTIFIER=74</code> executes VGG16 with RCA, <code>FUNCTION_IDENTIFIER=174</code> with PPA, and <code>FUNCTION_IDENTIFIER=274</code> with PPA4.</p>"},{"location":"core/","title":"Core","text":"<p>The Core contains the backbone for networking, cryptographic operations, and hardware acceleration techniques such as Bitslicing, GPU acceleration, and Vectorization.</p> <p>The Core is divided into the following sub-components:</p> SUB-COMPONENT DESCRIPTION Arch Contains the architecture-specific headers for Bitslicing and Vectorization Crypto Contains implementations of cryptographic operations such as AES and SHA CUDA Contains standalone CUDA implementations for matrix multiplications and convolutions Networking Handles socket communication and SSL/TLS connections between parties Other Contains microbenchmarks for cryptographic operations and basic operations, utilities for printing and debugging, and SSL certificates used by the nodes to establish secure connections <p>Tip</p> <p>We provide different configuration options to support various architectures. These options can be set in the <code>Makefile</code> or <code>config.h</code> file. It is worth experimenting with different configurations to find the best performance for a specific use case.</p>"},{"location":"core/#arch","title":"Arch","text":"<p>Bitslicing and Vectorization become more effective with larger register sizes. However, not all CPUs support wider registers and not all functions require high parallelism.  Arch contains headers for common X86 architectures such as SSE and AVX512. The headers contain efficient conversions for Bitslicing and Vectorization and map architecture-specific operations to generic functions. This way a user can write code that is architecture-agnostic, and compile the code with the appropriate flags to increase or decrease the level of parallelism. The <code>DATTYPE</code> config option defines the register size. For instance, setting <code>DATTYPE</code> to 512 will use AVX512 registers for Bitslicing and Vectorization, while setting it to 32 will use uint32_t registers. The following options are available.</p> Register Size Requirements Supported BITLENGTH Config Option 512 AVX512 16, 32, 64 <code>DATTYPE=512</code> 256 AVX2 16, 32, (64 with AVX512) <code>DATTYPE=256</code> 128 SSE 16, 32, (64 with AVX512) <code>DATTYPE=128</code> 64 None 64 <code>DATTYPE=64</code> 32 None 32 <code>DATTYPE=32</code> 16 None 16 <code>DATTYPE=16</code> 8 None 8 (Does not support all arithmetic instructions) <code>DATTYPE=8</code> 1 None 8,16,32,64 (Use only for boolean circuits) <code>DATTYPE=1</code> <p>The vectorization factor for a <code>DATTYPE</code> can be calculated as <code>DATTYPE</code>/ <code>BITLENGTH</code>. For instance, if <code>DATTYPE</code> is 256 and <code>BITLENGTH</code> is 32, each arithmetic instruction will be executed on 256/32=8 inputs in parallel, and each boolean operation will be executed 256 times in parallel. <code>DATTYPE=32</code> and <code>BITLENGTH=32</code> will not vectorize arithmetic operations and <code>DATTYPE=1</code> and <code>BITLENGTH=32</code>will not bitslice boolean operations.</p> <p>Warning</p> <p>Some BITLENGTHs may not be supported for all DATTYPEs. For instance, AVX2 does not support arithmetic optations on packed 64-bit integers. Check the Table above for supported BITLENGTHs for each DATTYPE. All nodes must use the same DATTYPE and BITLENGTH to ensure compatibility.</p> <p>References</p> <p>The architecture-specific headers for vectorization and Bitslicing are adapted from USUBA, MIT LICENSE.</p>"},{"location":"core/#crypto","title":"Crypto","text":"<p>Crypto contains cryptographic implementations for AES and SHA. The AES implementation is used to generate shared random numbers between parties and the SHA implementation is used to compare the views of the parties to ensure consistent messages. We support three different AES implementations for different architectures and two different SHA implementations.</p> Implementation Description Config Option AES (X86) Uses the AES-NI or VAES instruction set for AES encryption and decryption. This option is usually the fastest if the CPU supports AES-NI or VAES. VAES uses wider registers to increase parallelism. <code>RANDOM_ALGORITHM=2</code> AES (Bitslicing) A Bitsliced implementation of AES that does not require any special instruction set. <code>RANDOM_ALGORITHM=1</code> AES (OPENSSL) Uses the OpenSSL library for AES encryption and decryption. <code>USE_SSL_AES=1</code> and <code>RANDOM_ALGORITHM=2</code> SHA (X86) Uses the SHA-NI instruction set for SHA hashing if available. This option is usually the fastest if the CPU supports SHA-NI. IF SHA-NI is not available this option falls back to a non-accelerated implementation of SHA. <code>USE_ARM=0</code> SHA (ARM) Uses SHA instruction set for ARM CPUs. <code>USE_ARM=1</code> <p>Tip</p> <p>The <code>VERIFY_BUFFER</code> sets how many messages get accumulated before a hash is computed. Setting <code>VERIFY_BUFFER=0</code> will buffer all messages and compute a single hash at the end of the protocol. Usually, a smaller buffer provides the best results. Note that to ensure correctness, the following equation must hold \\(VERIFY\\_BUFFER \\cdot DATTYPE \\ge 512\\), since the SHA function requires at least 512 bits of inputs to compute a hash. Setting <code>VERIFY_BUFFER</code> to <code>512/DATTYPE</code> usually achieves the best performance.</p> <p>References</p> <p>The AES-NI implementation is adapted from AES-Brute-Force, Apache 2.0 LICENSE</p> <p>The bitsliced AES implementation is adapted from USUBA, MIT LICENSE.</p> <p>The SHA-256 implementation is adapted from SHA-Intrinsics, No License.</p>"},{"location":"core/#cuda","title":"CUDA","text":"<p>In the CUDA folder, we provide standalone implementations of matrix multiplications and convolutions. These need to be compiled separately and will then be linked automatically by setting the appropriate flags in the project's <code>Makefile</code> or <code>config.h</code>.  To compile, the node requires a CUDA-compatible GPU and the CUDA toolkit installed. The implementations also require the CUTLASS library that provides templated CUDA kernels. The executables can be built as follows. <pre><code># Dependencies for GPU acceleration\ngit clone https://github.com/NVIDIA/cutlass.git\n# Compile standalone executable for GPU acceleration\ncd core/cuda\nmake -j arch=sm_89 CUDA_PATH=/usr/local/cuda CUTLASS_PATH=/home/user/cutlass # Replace with you architecture, nvcc path and CUTLASS path\n</code></pre></p> <p>Note that some target architectures may not support datatypes such as uint16_t on the GPU. Thus, by default, some of these are commented out in our source files. If you want to use a BITLENGTH of 16 with GPU acceleration you can uncomment the appropriate lines in the <code>.cu</code> source files of the CUDA directory.</p> <p>We provide four different options for accelerating matrix multiplications and convolutions on the GPU. </p> Approach Description Config Option CPU Matrix Multiplication The matrix multiplication is handled on the CPU but utilizes optimizations such as cache tiling and transposing. <code>USE_CUDA_GEMM=0</code> GPU Matrix Multiplication Accelerates matrix multiplications on the GPU. Convolutions are split up in im2col layout changes handled by the CPU and only the matrix multiplication itself is outsourced. <code>USE_CUDA_GEMM=1</code> Optimized GPU Matrix Multiplication Similiar to <code>USE_CUDA_GEMM=1</code> but provides improved data transfer and scheduling on the GPU that should be faster on most architectures. <code>USE_CUDA_GEMM=3</code> NCHW GPU Convolution Handles the whole convolution operation on the GPU. This is usually faster than the previous approaches. Separate matrix multiplications are handled the same way as in <code>USE_CUDA_GEMM=3</code>. <code>USE_CUDA_GEMM=2</code> CHWN GPU Convolution Similiar to <code>USE_CUDA_GEMM=2</code> but uses a different layout for the input and output tensors. This layout may be faster on some architectures. <code>USE_CUDA_GEMM=4</code> <p>References</p> <p>CUDA GEMM and Convolution implementations are adapted from Cutlass, LICENSE and Piranha, MIT LICENSE.</p>"},{"location":"core/#networking","title":"Networking","text":"<p>In the networking folder, we provide implementations for socket communication and TCP/TLS connections between parties.  Each party has a sending and receiving thread for each party it communicates with. The threads have the following responsibilities.</p> <ul> <li>Sending Threads: The sending threads wait until the main thread signals a condition variable by invoking the <code>send()</code> method. This is either done manually or if the <code>SEND_BUFFER</code> is full.</li> <li>Receiving Threads: The receiving threads continuously buffer incoming messages and signal the main thread whenever the <code>RECV_BUFFER</code> is full. Whenever the main thread requires a new chunk of messages it calls the <code>receive</code> method which consumes the data if it is already buffered or blocks until the receiving thread buffered the required data.</li> </ul> <p>This execution model ensures that all communication between parties is utilizing parallelism and the main thread is only blocked when it needs to wait for a specific message to arrive before proceeding.</p> <p>The following configuration options are available for networking.</p> Option Description Config Option Encrypted Communication Encrypts messages between the parties using TLS implemented by OpenSSL <code>USE_SSL=1</code> Send Buffer Size Buffers <code>x</code> elements before sending a message. Setting <code>x=0</code> waits until all elements for a communication round are available. Each element is of size <code>DATTYPE</code> bits. <code>SEND_BUFFER=x</code> Receive Buffer Size Buffers <code>x</code> elements before signaling the main thread. Setting <code>x=0</code> only signals the main thread when all elements for a communication round are available. Each element is of size <code>DATTYPE</code> bits. <code>RECV_BUFFER=y</code> <p>Tip</p> <p>Experimenting with different buffer sizes can improve performance significantly. The default option of <code>SEND_BUFFER=10000</code> and <code>RECV_BUFFER=10000</code> showed good performance in various experiments.</p>"},{"location":"datatypes/","title":"Datatypes","text":"<p>Datatypes provide a protocol-agnostic interface to implement high-level MPC functions. For instance, a matrix multiplication can be implemented by using a dot product primitive of a protocol in a black box manner. For this purpose, the <code>Additive_Share</code> class provides generic interfaces that all protocols need to implement. Similarly, the <code>XOR_Share</code> class provides an interface for operations in the boolean domain.</p> <p>The datatypes <code>sint</code> and <code>Bitset</code> are containers that are used by functions such as comparisons to efficiently switch between Bitslicing and Vectorization during share conversion. In most cases, these datatypes do not need to be used directly, as functions, such as comparisons and ReLUs, are already implemented and can be imported as modules from the <code>programs/functions</code> folder.</p> <p>Finally, the <code>FloatFixedConverter</code> class converts floating point numbers to fixed point numbers and vice versa. One conversion is typically required before secretly sharing a floating point number and the other conversion is required after revealing the result of a computation to convert the fixed point number back to a floating point number.</p> <p>The following tables provide an overview of the datatypes and their functions.</p> Datatype Description <code>Additive_Share</code> A secret share in the arithmetic domain <code>XOR_Share</code> A secret share in the boolean domain <code>sint</code> A container of size <code>BITLENGTH</code>storing <code>Additive_Shares</code> <code>Bitset</code> A container of size <code>BITLENGTH</code> storing <code>XOR_Shares</code> <code>FloatFixedConverter</code> A static struct to convert floating point numbers to fixed point numbers and vice versa"},{"location":"datatypes/#implementing-functions-with-datatypes","title":"Implementing Functions with Datatypes","text":"<p>The tutorials in the <code>programs/tutorials</code> folder provide examples of how to implement functions using the provided datatypes. Below is a quick example of how to operate on <code>Additive_Share</code> datatypes. As one can see, shares behave similarly to normal integers and can be used with STL containers such as <code>std::vector</code>.</p> <pre><code>template &lt;typname Share&gt; \nvoid add_and_mult()\n{\n    using A = Additive_Share&lt;DATATYPE, Share&gt;; \n    A a[] = {A(2), A(3)}; // Initialize shares from public values\n    A b[] = {A(4), A(5)};\n    auto c = a[0] + b[0]; // Add two shares\n    A d = c.prepare_mult(a[1] + b[1]); // Prepare multiplication\n    Share::communicate();\n    d.complete_mult(); // Complete multiplication\n}\n</code></pre>"},{"location":"datatypes/#conversions-between-bitslicing-and-vectorization","title":"Conversions between Bitslicing and Vectorization","text":"<p>As HPMPC uses Bitslicing for boolean operations and Vectorization for arithmetic operations, converting a single share from one domain to the other is not possible: A bitsliced variable with a register length of size <code>DATTYPE</code> stores <code>DATTYPE</code> bits from <code>DATTYPE</code> independent values, while a vectorized variable stores <code>DATTYPE</code> bits from <code>DATTYPE/BITLENGTH</code> independent values. Thus a conversion between the two domains requires aggregating <code>BITLENGTH</code> shares in a container.  This way, a container in both domains holds <code>DATTYPE*BITLENGTH</code> bits from <code>DATTYPE</code> independent values.</p> <p>For this purpose, the <code>sint</code> and <code>Bitset</code> classes are provided. Functions such as comparisons and ReLUs operate on these containers and are invoked using the <code>pack_additive</code> function that converts a contiguous array of <code>Additive_Share</code> to a <code>sint</code> container, performs the operation, and converts the result back to an array of <code>Additive_Shares</code>. In practice, users do not need to interact with the <code>sint</code> and <code>Bitset</code> containers directly but can use the pre-implemented functions from the <code>programs/functions</code> folder.</p>"},{"location":"datatypes/#fixed-point-arithmetic","title":"Fixed Point Arithmetic","text":"<p>To efficiently handle decimal numbers, HPMPC uses fixed-point arithmetic. The <code>FloatFixedConverter</code> class provides functions to convert floating point numbers to fixed point numbers and vice versa.  This conversion utilizes the <code>FRACTIONAL</code> config option that defines the number of bits used for the fractional part of a fixed point number.  By increasing the <code>FRACTIONAL</code> config option, the precision of the fixed point numbers can be increased at the cost of a lower integer range.</p> <p>After converting a floating point number to a fixed point number, the fixed point number can be secretly shared and after revealing the result, the fixed point number can be converted back to a floating point number. The following example demonstrates how to convert a floating point number to a fixed point number and vice versa. </p> <pre><code>// Plaintext fixed point value with FRACTIONAL bits of precision:\nUINT_TYPE fixed_point_value = FloatFixedConverter&lt;float, INT_TYPE, UINT_TYPE, FRACTIONAL&gt;::float_to_ufixed(3.5f); \n\n// Convert back:\nfloat float_value = FloatFixedConverter&lt;float, INT_TYPE, UINT_TYPE, FRACTIONAL&gt;::ufixed_to_float(fixed_point_value);\n</code></pre> <p>Tip</p> <p>Increasing the <code>BITLENGTH</code> config option can help to increase the range of fixed point numbers that can be represented. Also, different truncation approaches can be utilized. For instance, <code>TRUNC_APPROACH=0</code> requires a slack of a few bits to avoid truncation errors while <code>TRUNC_APPROACH=1</code>, <code>TRUNC_APPROACH=2</code> and <code>TRUNC_APPROACH=3</code> require a slack of only 0-1 bits but introduce additional communication overhead.</p>"},{"location":"measurements/","title":"Measurements","text":"<p>Out of the box, HPMPC provides a set of benchmarks for different MPC primitives, high-level functions, and use cases.  Most benchmarks are contained in <code>programs/benchmarks</code>.  In order to run a benchmark, first obtain the <code>FUNCTION_IDENTIFIER</code> of the benchmark you want to run which is defined in the source file of the respective benchmark. Typically, a benchmark relies on the <code>NUM_INPUTS</code> config option. To compile a benchmark for a specific number of inputs and protocols, run the following command: <pre><code>make -j PARTY=&lt;party_id&gt; FUNCTION_IDENTIFIER=&lt;function_identifier&gt; NUM_INPUTS=&lt;num_inputs&gt;\n</code></pre> In the following lines, we explain how to run and interpret the results of a benchmark and to automate benchmarking of multiple configurations.</p>"},{"location":"measurements/#obtain-benchmark-results","title":"Obtain Benchmark Results","text":"<p>As with other MPC programs, benchmarks usually benefit from vectorization and parallelization. To obtain the best performance, it is recommended to run benchmarks with the maximum <code>DATTYPE</code> and <code>NUM_PROCESSES</code> that the hardware supports. In many cases, also <code>SPLITROLES</code> can be used to improve performance. Since the introduced parallelism increases the number of performed operations it is important to consider the number of inputs and the parallelization factor when interpreting the results. The parallelization factor can usually be calculated as follows:</p> <p>parallelization_factor \\(=\\) <code>DATTYPE/BITLENGTH</code> \\(\\cdot\\) <code>NUM_PROCESSES</code> \\(\\cdot\\) <code>SPLITROLES_FACTOR</code> # Boolean (only) circuits do not require division by BITLENGTH</p> <p>Total number of inputs processed \\(=\\) <code>NUM_INPUTS</code> \\(\\cdot\\) parallelization_factor</p> <p>Note that Boolean operations that would usually be executed on single bits have a higher parallelization factor than arithemtic operations that would usually already be executed on registers with multiple bits (e.g. 32 for uint32).  Thus, for a circuit such as AES, that only utilizes boolean operations, the parallelization factor is significantly higher than for a circuit that also uses arithmetic operations. For mixed circuits, the parallelization factor of arithmetic operations applies as the program typically starts with and ends with arithmetic shares.</p> <p>The SplitRoles factor is 1 for <code>SPLITROLES=0</code>, 6 for <code>SPLITROLES=1</code>, and 24 for <code>SPLITROLES=2</code> and <code>SPLITROLES=3</code>. The reason for this is that SPLITROLES executes the same executable for all party permutations in parallel. For 3PC protocols, there are 6 permutations and for 4PC protocols, there are 24 permutations.</p>"},{"location":"measurements/#automate-benchmarking","title":"Automate Benchmarking","text":"<p>To automate benchmarking of multiple configurations, we provide configuration files in <code>measurements/configs/</code>. A configuration file contains a list of different arguments that are passed to the Makefile.  Each combination of arguments can be then compiled and executed by the provided scripts. The following <code>.conf</code> file tests 12 different protocols and 3 different input sizes for the benchmark with <code>FUNCTION_IDENTIFIER=1</code>. This results in 36 different configurations. <pre><code>FUNCTION_IDENTIFIER=1\nPROTOCOL=1,2,3,4,5,6,7,8,9,10,11,12\nPROCESS_NUM=32\nDATTYPE=512\nBITLENGTH=1\nPRE=0\nNUM_INPUTS=10000,100000,1000000\n</code></pre></p> <p>To run the benchmark for all configurations, execute the following command: <pre><code>./measurements/run_config.py measurements/configs/&lt;config_file&gt;.conf\n</code></pre> By default, the script will run the benchmark for all parties locally. The following options can be used to specify the party and IP addresses:</p> Option Description -p Party identifier -a IP address for party 0 -b IP address for party 1 -c IP address for party 2 -d IP address for party 3 -s <code>SPLITROLES</code> Identifier (0, 1, 2, or 3) -g Numbers of GPUs to use (0 for CPU only) --override Override config options (key=value) -i Number of iterations per run <p>The results are stored in the <code>measurements/logs/</code> directory with the name of the test and a timestamp.  Setting the number of iterations per run is useful to later compute means and standard deviations of the results.  Note that the config file may set a DATTYPE that is not supported by the target hardware. In this case, the override option allows specifying a different value than the one in the config file. The following command is an example for running a benchmark in a distributed setting on a specific hardware configuration with one GPU, AVX-2 (<code>DATTYPE=256</code>) support, 10 iterations per run, <code>SPLITROLES=1</code> (6 permutations), and <code>PROCESS_NUM=2,4</code>:</p> <pre><code>./measurements/run_config.py -p &lt;party_id&gt; -a &lt;ip_address_a&gt; -b &lt;ip_address_b&gt; -c &lt;ip_address_c&gt; -d &lt;ip_address_d&gt; -s 1 -g 1 -i 10 measurements/configs/&lt;config_file&gt;.conf --override DATTYPE=256 PROCESS_NUM=2,4\n</code></pre> <p>Finally, the results can be parsed using the <code>parse_logs.py</code> script. The script outputs a <code>.csv</code> table with the results of the benchmark and automatically computes useful stats such as the runtime in seconds, the size of messages sent and received in MB, and the throughput in Gbps. If applicable, the script also computes the number of operations per second, accuracy, and the number of tests that passed. To compute the number of operations per second, the <code>.conf</code> file needs to provide <code>NUM_INPUTS</code>, <code>DATTYPE</code>, and <code>BITLENGTH</code> to compute the formula from above automatically. The script can either be executed for a single log file or on a directory containing multiple log files.</p> <pre><code>./measurements/parse_logs.py measurements/logs/&lt;log_file&gt;.log\n</code></pre> <p>The neural network specific script <code>parse_layers.py</code> script collects statistics for each neural network layer and outputs a csv table with the results for each model. The script can be executed for a single log file or on a directory containing multiple log files. The <code>run_pretrained.py</code> executes all pretrained neural network models from <code>Pygeon</code> with different configurations regarding truncation, bitlength, and the number of fractional bits.</p>"},{"location":"nn/","title":"Neural Networks","text":"<p><code>HPMPC</code> provides a high-level interface for performing secure inference of neural networks.</p> <ul> <li><code>PIGEON</code> is a templated inference engine for private inference of neural networks and handles the data flow between layers.</li> <li><code>PyGEON</code> is a Python library that allows users to export models and datasets from <code>PyTorch</code> to <code>PIGEON</code>.</li> <li><code>Programs/NN</code> orchestrates the execution of <code>PIGEON</code> and the MPC backend.</li> </ul>"},{"location":"nn/#pigeon","title":"PIGEON","text":"<p><code>PIGEON</code> is a templated inference engine for private inference of neural networks. Models and datasets can be exported from <code>PyTorch</code> to <code>PIGEON</code>. <code>PIGEON</code> then performs a forward pass on the model and dataset by relying on high-level functions provided by HPMPC.  <code>PIGEON</code> consists of two main components: <code>Architectures</code> and <code>Headers</code>.</p>"},{"location":"nn/#architectures","title":"Architectures","text":"<p><code>Architectures</code> for neural networks are defined in the <code>architectures</code> directory.  Out of the box <code>PIGEON</code> supports multiple ResNet and Convolutional Neural Network (CNN) architectures such as AlexNet, VGG, and LeNet.</p> <p>The following is an example of the LeNet architecture as defined in <code>CNNs.hpp</code>. As one can see, layers can be defined in a similar manner to <code>PyTorch</code>. Architectures such as ResNets can also be defined in a programmatic manner as seen in <code>ResNet.hpp</code>. The example below also shows a ReLU layer with reduced bitlength.  In the example, only bits 8-16 are used for the sign bit extraction required by ReLU thus reducing communication complexity at the cost of accuracy. Related Work such as <code>Hummingbird</code> can be used to identify the optimal bitlength for each layer.</p> <pre><code>template &lt;typename T&gt;       \nclass LeNet : public SimpleNN&lt;T&gt;\n{\n    public:\n    LeNet(int num_classes)\n    {\n        this-&gt;add(new Conv2d&lt;T&gt;(1,6,5,1,2));\n        this-&gt;add(new ReLU&lt;T&gt;());\n        this-&gt;add(new AvgPool2d&lt;T&gt;(2,2));\n        this-&gt;add(new Conv2d&lt;T&gt;(6,16,5,1,0));\n        this-&gt;add(new ReLU&lt;T,8,16&gt;()); // ReLU with reduced bitlength\n        this-&gt;add(new AvgPool2d&lt;T&gt;(2,2));\n        this-&gt;add(new Flatten&lt;T&gt;());\n        this-&gt;add(new Linear&lt;T&gt;(400,120));\n        this-&gt;add(new ReLU&lt;T&gt;());\n        this-&gt;add(new Linear&lt;T&gt;(120,84));\n        this-&gt;add(new ReLU&lt;T&gt;());\n        this-&gt;add(new Linear&lt;T&gt;(84,num_classes));\n    }\n};\n</code></pre>"},{"location":"nn/#headers","title":"Headers","text":"<p>Layers are implemented in a generic manner in the <code>headers</code> directory.  <code>PIGEON</code> itself only performs non-arithmetic operations such as matrix transposition, reshaping, and handling the data flow between layers. All arithmetic operations are performed by high-level functions provided by <code>HPMPC</code>. This modular design allows for an easy addition of new layers and neural network architectures to <code>PIGEON</code> without knowledge of the MPC backbone.  <code>PIGEON</code> could potentially be used with other MPC backends as long as they provide the required high-level functions with the interface required by <code>PIGEON</code>.</p> <p>The following is a list of layers currently implemented in <code>PIGEON</code>:</p> Layer Description Conv2d 2D Convolution Linear Fully Connected Layer ReLU Rectified Linear Unit Softmax Softmax (Argmax) Activation AvgPool2d 2D Average Pooling MaxPool2d 2D Max Pooling AdaptiveAvgPool2d 2D Adaptive Average Pooling BatchNorm2d 2D Batch Normalization BatchNorm1d 1D Batch Normalization Flatten Flatten Layer"},{"location":"nn/#pygeon","title":"PyGEON","text":"<p><code>PyGEON</code> is a Python library that allows users to export models and datasets from <code>PyTorch</code> to <code>PIGEON</code>. The library provides the following functionalities.</p> <ul> <li>Download, transform, and edit datasets in PyTorch and export them as <code>.bin</code> files</li> <li>Train models in PyTorch and export them as <code>.bin</code> files</li> <li>Import existing model parameters as <code>.pth</code> files and export them as <code>.bin</code> files</li> </ul> <p>The generated <code>.bin</code> files are compatible with <code>PIGEON</code> and can be used to achieve similar accuracy as the original model in PyTorch.</p>"},{"location":"nn/#train-and-export-a-model","title":"Train and Export a Model","text":"<p>A single line of code suffices to train a model in PyTorch and export it to <code>PIGEON</code>. The following command trains an AlexNet model on the CIFAR-10 dataset for 30 epochs and exports the model and datasets as <code>.bin</code> files.</p> <pre><code> python main.py --action train --export_model --export_dataset --transform standard --model AlexNet --num_classes 10 --dataset_name CIFAR-10 --modelpath ./models/alexnet_cifar --num_epochs 30 --lr 0.01 --criterion CrossEntropyLoss --optimizer Adam\n</code></pre> <p>The <code>main.py</code> script provides the following functionalities:</p> Argument Description <code>--action</code> Action to perform on the model: <code>train</code>, <code>import</code>, <code>train_all (for training all predifined model architectures)</code>, <code>none</code> (e.g. for only exporting the dataset) <code>--export_model</code> Export the model as a <code>.bin</code> file for <code>PIGEON</code> <code>--export_dataset</code> Export the test dataset as a <code>.bin</code> file for <code>PIGEON</code> <code>--model</code> Model architecture as defined in <code>cnns.py</code> <code>--num_classes</code> Number of classes in the dataset <code>--dataset_name</code> Name of the dataset as defined in <code>data_load.py</code> <code>--modelpath</code> Path to save the model to (if --export_model is set) or load the model from (if --action is <code>import</code>) <code>--num_epochs</code> Number of epochs to train the model <code>--lr</code> Learning rate for the optimizer <code>--criterion</code> Loss function to use <code>--optimizer</code> Optimizer to use <code>--transform</code> Type of transformation to apply to the dataset: <code>custom</code> or <code>standard</code> <p>New model architectures can be added to <code>cnns.py</code> and new datasets can be added to <code>data_load.py</code> to extend the functionality of <code>PyGEON</code>.</p>"},{"location":"nn/#import-pretrained-models","title":"Import Pretrained Models","text":"<p>We provide a set of pretrained models that can be imported to <code>PIGEON</code> using the <code>download_pretrained.py</code> script. </p> <p>The following command downloads all pretrained models to the <code>models/pretrained</code> folder and all datasets to the <code>data/datasets</code> folder. <pre><code>python download_pretrained.py all\n</code></pre> Note that downloading all models requires a few GB of disk space. Thus, we also provide the option to download some of the models and datasets with the following options.</p> Argument Description <code>all</code> Download all models and datasets <code>single_model</code> Download VGG16, trained on CIFAR-10 (standard transform) <code>cifar_adam_001_pretrained</code> Download several models, trained on CIFAR-10 with Adam optimizer and lr=0.01 <code>cifar_adam_005_pretrained</code> Download several models, trained on CIFAR-10 with Adam optimizer and lr=0.05 <code>cifar_sgd_001_pretrained</code> Download several models, trained on CIFAR-10 with SGD optimizer and lr=0.01 <code>lenet5_pretrained</code> Download LeNet5, trained on MNIST (different transforms) <code>datasets</code> Download all datasets <p>The different options can be combined to download multiple models and datasets at once. <pre><code>python download_pretrained.py single_model datasets # Downloads VGG16 and all datasets\n</code></pre></p>"},{"location":"nn/#nn","title":"NN","text":"<p><code>Programs/NN</code> orchestrates the execution of <code>PIGEON</code> and the MPC backend. The <code>NN</code> program provides the following functionalities.</p> <ul> <li>Load a model and dataset from <code>PyGEON</code> using environment variables.</li> <li>Obtain the model parameters and the dataset from the right party and secretly share them.</li> <li>Define the model architecture and dataset dimensions for performing a forward pass.</li> <li>Perform a forward pass on the model using <code>PIGEON</code> as the inference engine and <code>HPMPC</code> as the MPC backend.</li> </ul>"},{"location":"nn/#evaluate-a-model","title":"Evaluate a Model","text":"<p>To evaluate a model, the program first assigns a <code>FUNCTION_IDENTIFIER</code> to the model architecture and dataset dimensions.  For instance, the following line defines that the VGG model is evaluated when the FUNCTION_IDENTIFIER is set to 74. <pre><code>#if FUNCTION_IDENTIFIER == 74 \n    int n_test = NUM_INPUTS*BASE_DIV, ch = 3, h = 32, w = 32, num_classes = 10; // CIFAR-10 input dimensions\n    auto model = VGG&lt;modeltype&gt;(num_classes); // CNN architecture as defined in CNNs.hpp of PIGEON\n#endif\n</code></pre></p> <p>A custom model can be evaluated by defining a new architecture in <code>CNNs.hpp</code> or <code>ResNet.hpp</code> and assigning a new <code>FUNCTION_IDENTIFIER</code> to the model in <code>NN.hpp</code>. The program ensures that each process and vector handles a separate part of the dataset and prints the accuracy of its classifications in the terminal.</p>"},{"location":"nn/#secret-sharing-of-model-parameters-and-data","title":"Secret Sharing of Model Parameters and Data","text":"<p>The program checks which party is responsible for sharing the model parameters and which party is responsible for sharing the dataset.  The parties can be specified with the <code>MODELOWNER</code> and <code>DATAOWNER</code> config options. <code>MODELOWNER=P_0</code> and <code>DATAOWNER=P_1</code> specify that party 0 is responsible for sharing the model and party 1 is responsible for sharing the dataset. Setting <code>MODELOWNER=-1</code> and <code>DATAOWNER=-1</code> skips secret sharing which is useful for benchmarking.</p> <p>For the node acting as the model owner, the program loads the model parameters from the <code>.bin</code> file as defined by the environment variables <code>MODEL_DIR</code> and <code>MODEL_FILE</code>. The model at <code>MODEL_DIR/MODEL_FILE</code> is loaded and its parameters are secretly shared. Below is an example of how to set the environment variables for the VGG16 model trained on CIFAR-10. <pre><code>export MODEL_DIR=nn/Pygeon/models/pretrained\nexport MODEL_FILE=VGG16_CIFAR-10_standard.bin\n</code></pre></p> <p>For the node acting as the data owner, the program loads the dataset from the <code>.bin</code> file as defined by the environment variables <code>DATA_DIR</code>, <code>SAMPLES_FILE</code>, and <code>LABELS_FILE</code>. The samples at  <code>DATA_DIR/SAMPLES_FILE</code> are loaded and secretly shared. Below is an example of how to set the environment variables for the CIFAR-10 dataset. <pre><code>export DATA_DIR=nn/Pygeon/data/datasets\nexport SAMPLES_FILE=CIFAR-10_standard_test_images.bin\nexport LABELS_FILE=CIFAR-10_standard_test_labels.bin\n</code></pre></p> <p>Note that each party that requires obtaining the correct accuracy needs the labels of the dataset in plaintext. The environment variables can be adjusted without the need to recompile the program. Also, the program prints in the terminal whether the model and dataset were loaded correctly.</p>"},{"location":"parallel-model/","title":"The Vectorized Programming Model","text":"<p>HPMPC maximizes resource utilization and minimizes thread synchronization by using a vectorized programming model. Each operation performed on secret shares by a program is inherently parallelized by using SIMD instructions and multiple processes. To illustrate the vectorized programming model, consider the following example:</p> <pre><code>using A = Additive_Share&lt;DATATYPE, Share&gt;;\nA x = A(1);\nA y = A(2);\nA z = x + y;\n</code></pre> <p>The code snippet looks like a simple addition of two shares without any parallelization. However, the addition is performed multiple times in parallel by using vectorization and multiprocessing. Suppose the <code>DATTYPE=512</code>, <code>BITLENGTH=32</code>, and <code>NUM_PROCESSES=32</code>. In this case, each Additive_Share object holds 16 independent secret shares, and the addition is performed on all 16 shares in parallel on each of the 32 processes.  Thus, in total, 512 secret shares are added in parallel. Clearly, under these circumstances, assigning the same value to all of the 512 shares as shown above would be a waste of resources. Instead, parties can assign different values to each of the 16 independent shares and 32 processes to perform 512 unique additions in parallel.</p> <pre><code>using A = Additive_Share&lt;DATATYPE, Share&gt;;\nconst int vectorization_factor=DATTYPE/BITLENGTH;\nDATATYPE x_vectorized_values, y_vectorized_values;\nUINT_TYPE x_values[vectorization_factor];\nUINT_TYPE y_values[vectorization_factor];\nfor(int i=0; i&lt;vectorization_factor; i++)\n{\n    x_values[i] = i+process_offset; // dummy value assignment\n    y_values[i] = i+process_offset+vectorization_factor; // dummy value assignment\n}\northogonalize_arithmetic(x_values, x_vectorized_values,1);\northogonalize_arithmetic(y_values, y_vectorized_values,1);\nA x = A::get_share_from_public_dat(vectorized_values);\nA y = A::get_share_from_public_dat(vectorized_values);\nA z = x + y;\n</code></pre> <p>The code snippet above assigns a unique value to each of the 512 shares by using the <code>orthogonalize_arithmetic</code> function to convert an array of unsigned integers to a vectorized value that matches the register size specified by <code>DATTYPE</code>. It also uses the globally available <code>process_offset</code> to determine the unique process ID and assign different values depending on the process ID. The <code>process_offset</code> also works correctly with SPLITROLES across executables.</p>"},{"location":"parallel-model/#vectorized-secret-sharing-and-revealing","title":"Vectorized Secret Sharing and Revealing","text":"<p>A full example would typically also involve secret sharing as opposed to setting all values from public data. <pre><code>using A = Additive_Share&lt;DATATYPE, Share&gt;;\nconst int vectorization_factor=DATTYPE/BITLENGTH;\nDATATYPE x_vectorized_values, y_vectorized_values;\n\n#if PSELF == P_0 // Code block only gets executed by party 0\nfor(int i=0; i&lt;vectorization_factor; i++)\n{\n    UINT_TYPE x_values[vectorization_factor];\n    x_values[i] = i+process_offset; // dummy value assignment\n    orthogonalize_arithmetic(x_values, x_vectorized_values,1);\n}\n#elif PSELF == P_1 // Code block only gets executed by party 1\nfor(int i=0; i&lt;vectorization_factor; i++)\n{\n    UINT_TYPE y_values[vectorization_factor];\n    y_values[i] = i+process_offset+vectorization_factor; // dummy value assignment\n    orthogonalize_arithmetic(y_values, y_vectorized_values,1);\n}\n#endif\n\nA x,y;\nx.template prepare_receive_from&lt;P_0&gt;(vectorized_values);\ny.template prepare_receive_from&lt;P_1&gt;(vectorized_values);\nShare::communicate();\nx.template complete_receive_from&lt;P_0&gt;();\ny.template complete_receive_from&lt;P_1&gt;();\nA z = x + y;\n</code></pre></p> <p>The shares are now vectorized throughout the entire program, thus inherently benefitting by from all subsequent operations being parallelized. For full secrecy, the inputs provided in the code blocks that are exclusive to a party can come from an external source such as a local file. When revealing the result, the shares are also vectorized and the result is revealed in a vectorized manner. The vectorized result can be converted back to an array of unsigned integers by using the <code>unorthogonalize_arithmetic</code> function.</p> <pre><code>A z = x.prepare_mult(y);\nShare::communicate();\nz.complete_mult_without_trunc();\nz.prepare_reveal_to_all();\nShare::communicate();\nDATATYPE vectorized_result = z.complete_reveal_to_all();\nUINT_TYPE result_values[vectorization_factor];\nunorthogonalize_arithmetic(vectorized_result, result_values,1);\nfor(int i=0; i&lt;vectorization_factor; i++)\n    std::cout &lt;&lt; result_values[i] &lt;&lt; std::endl;\n</code></pre>"},{"location":"parallel-model/#minimizing-communication-rounds","title":"Minimizing Communication Rounds","text":"<p>To avoid overhead from parsing and interpreting circuit representations, HPMPC entrusts developers to trigger communication rounds explictly. Note that implictly, messages are sent continuously between the parties whenever the <code>SEND_BUFFER</code> has been filled to interleave communication and computation.  However, to explictly send messages even when the <code>SEND_BUFFER</code> is not full, the <code>Share::communicate()</code> function is used. The function gurantees that all messages of a communication round get sent and should be called at the end of each communication round.</p> <p>The previous examples illustrated how the vectorized programming model can be used to parallelize operations on secret shares. However, in addition to vectorizing individual operations, multiple operations should also be grouped together to minimize the number of communication rounds between parties.  Assume that x,y, and z are now arrays of size 100. Observe that the following code snippet requires the same number of communication rounds as the previous example, but performs 100 times as many operations. </p> <pre><code>for(int i=0; i&lt;100; i++)\n    z[i] = x[i].prepare_mult(y[i]);\nShare::communicate(); // End of first communication round\nfor(int i=0; i&lt;100; i++)\n    z[i].complete_mult_without_trunc();\n\nDATATYPE vectorized_result_values[100];\nUINT_TYPE result_values[100][vectorization_factor];\n\nfor(int i=0; i&lt;100; i++)\n    z[i].prepare_reveal_to_all();\nShare::communicate(); // End of second communication round\nfor(int i=0; i&lt;100; i++)\n    vectorized_result_values[i] = z[i].complete_reveal_to_all();\nunorthogonalize_arithmetic(vectorized_result_values, result_values,100);\n\nfor(int i=0; i&lt;100; i++)\n    for(int j=0; j&lt;vectorization_factor; j++)\n        std::cout &lt;&lt; result_values[i][j] &lt;&lt; std::endl;\n</code></pre> <p>The predefined functions of HPMPC are designed to minimize the number of communication rounds between parties by letting the user provide contiguous arrays of secret shares as input along with the length and in some cases batch size. This way, the program can perform for instance multiple independent maximum operations on separate arrays in parallel without increasing the number of communication rounds compared to a single maximum operation.</p>"},{"location":"programs/","title":"Programs","text":"<p><code>Programs</code> define high-level functions and workloads that can be executed in an MPC-generic manner by using the provided <code>Datatypes</code>. Out of the box, HPMPC provides the following types of programs:</p> <ul> <li><code>Benchmarks</code>: Programs that are used to measure the performance of the MPC protocols and functions.</li> <li><code>Functions</code>: Programs that implement high-level functions such as comparisons, ReLUs, and matrix multiplications.</li> <li><code>Tests</code>: Programs that test the correctness of implemented functions and MPC primitives.</li> <li><code>Tutorials</code>: Programs that provide examples on how to implement functions using the provided datatypes.</li> </ul> <p>Especially, the <code>Functions</code> folder is relevant for users who want to implement their own program as multiple useful subroutines are already implemented by the the framework.</p> <p>Tip</p> <p>Before writing a new program, the tutorials in the <code>programs/tutorials</code> folder act as a starting point to understand how to implement functions using the provided datatypes.</p>"},{"location":"programs/#import-existing-functions-as-modules","title":"Import Existing Functions as Modules","text":"<p>The <code>Functions</code> folder contains a variety of high-level functions that are implemented in a protocol-agnostic manner. These functions can be imported as modules in a new program by including the corresponding header files. Below is an example of different functions that can be imported as modules.</p> Header Description <code>GEMM.hpp</code> Functions for matrix multiplication and matrix-vector multiplications <code>Comparison.hpp</code> Functions for comparisons such as less than zero, or equal to zero Different truncation headers Functions for truncating fixed point numbers using approaches such as probabilistic rounding, or deterministic rounding Different adder headers Functions to compute the carry-out or the sum of two boolean shares <code>ReLU.hpp</code> Functions for computing the ReLU function <code>prob_div.hpp</code> Functions for dividing a fixed point share by a public value probabilistically <p>Suppose a user wants to implement an average of two sets of numbers The following code snippet demonstrates how to utilize the predfined <code>prob_div</code> function to compute the two batches using only a single communication round. <pre><code>using A = Additive_Share&lt;DATATYPE, Share&gt;;\nA sum1 = A(0) + A(1) + A(2) + A(3) + A(4) + A(5) + A(6) + A(7) + A(8) + A(9);\nA sum2 = A(1) + A(3) + A(5) + A(7) + A(9) + A(11) + A(13) + A(15) + A(17) + A(19);\nA averages[] = {sum1, sum2};\nprepare_prob_div(averages[0], 10); // output, denominator\nprepare_prob_div(averages[1], 10);\nShare::communicate();\ncomplete_prob_div(averages, 2, 10); // output, batch size, denominator\n</code></pre></p> <p>Other functions follow a similar pattern. In order to reduce communication rounds, the user needs to provide a contiguous block of shares that are processed all at once. Functions such as ReLU and comparisons also handle the share conversion between the boolean and arithmetic domains internally and provide both inputs and outputs in the arithmetic domain. The tutorials show how the <code>pack_additive</code> function can be used to convert a contiguous array of <code>Additive_Share</code> to a <code>sint</code> container, perform the operation, and convert the result back to an array of <code>Additive_Shares</code>.</p>"},{"location":"programs/#execute-predefined-programs","title":"Execute Predefined Programs","text":"<p>Predefined Programs can be executed by setting the <code>FUNCTION_IDENTIFIER</code> config option in the <code>Makefile</code> or <code>config.h</code> file. The <code>protocol_executer.hpp</code> file contains a mapping of function identifiers to the corresponding functions that are executed by an executable. For instance, the following line in <code>protocol_executer.hpp</code> maps the function identifiers 0-7 to the benchmarks in the <code>bench_basic_primitives.hpp</code> file. <pre><code>#if FUNCTION_IDENTIFIER &lt; 8\n#include \"programs/benchmarks/bench_basic_primitives.hpp\"\n</code></pre> By inspecting the <code>bench_basic_primitives.hpp</code> file, one can see that each function_identifer from 0-7 corresponds to a different benchmark function.</p>"},{"location":"programs/#implement-custom-programs","title":"Implement Custom Programs","text":"<p>To implement a custom program, the user needs to create a new header file in the <code>programs</code> folder and include the header file in the <code>protocol_executer.hpp</code> file when the function identifier is set to a chosen value. For additional details on how to implement a custom program, the tutorial <code>programs/tutorials/YourFirstProgram.hpp</code> should get users started and provides examples on how to import modules and datatypes to implement a custom program.</p>"},{"location":"protocols/","title":"Protocols","text":"<p>The <code>Protocols</code> folder contains multiple MPC protocols that implement MPC primitives on a high level by utilizing templating and functions provided by the <code>Core</code>.</p> <p>Each protocol requires an init-Protocol and an actual protocol. The init-Protocol only describes the communication pattern of each primitive and the actual protocol implements the computation. Since HPMPC supports different register sizes in a generic way, the protocols operate on templated datatypes. Protocols that require a preprocessing phase can also implement a post-protocol. The post-protocol is executed after the preprocessing phase.</p> <p>The protocols only need to implement a few basic primitives such as secret sharing, revealing, addition, share conversion, dot products, and multiplications.  HPMPC implements several functions on top of these basic primitives to implement several protocols such as comparisons or matrix multiplication, and provides unit tests to ensure that the protocols are implemented correctly. </p>"},{"location":"protocols/#adding-a-new-protocol","title":"Adding a new protocol","text":"<p>The init protocol, actual protocol, and optional post protocol need to be defined in separate classes.  Whether it is useful to implement a joint class for all parties or a separate class for each party depends on the difference between the computation of each party. A minimal example requires implementing the following functions for the Init and the actual protocol.</p> <ul> <li><code>prepare_receive_from()</code> and <code>complete_receive_from()</code> to secretly share a value</li> <li><code>prepare_reveal_to_all()</code> and <code>complete_reveal_to_all()</code> to reveal a secret</li> </ul> <p>With these primitives in place, secret sharing and revealing can be tested. The other primitives can then be implemented and tested step by step.</p> <p>Note</p> <p>All protocols require identical function signatures to be later used as a template. The function signatures are defined by all previous protocols which may serve as references. HPMPC uses compile-time polymorphism using templates for improved performance and thus does not provide a base class for the protocols.</p> <p>After implementing the source files for a protocol, the files need to be referenced in <code>Protocols.h</code> and a <code>PROTOCOL</code> identifier needs to be defined. In case a party also requires the implementation of a post protocol, the post protocol needs to be referenced in <code>config.h</code>.</p> <p>Tip</p> <p>Examples of several protocols can be found in the <code>Protocols</code> folder. Especially the <code>Replicated</code> and <code>Sharemind</code> protocols are good starting points for new protocols.</p>"},{"location":"protocols/#implementation","title":"Implementation","text":"<p>A protocol is written from the perspective of an individual share. For instance a protocol where \\(P_i\\) holds shares \\(v_i\\) and \\(v_{i+1}\\) of a value \\(v\\) might look like this:</p> <pre><code>template &lt;typename Datatype&gt;\nclass ProtocolXYZ_Share{\nprivate:\n    Datatype i;\n    Datatype ip1;\npublic:\n...\n</code></pre> <p>Each primitive such as multiplication and addition can then be implemented as a function of this class. Note that local operations are also provided by templates since the <code>+,-,*</code> overloads are not provided for all possible datatypes. <pre><code>template &lt;typename func_add&gt;\nProtocolXYZ_Share Add(ProtocolXYZ_Share b, func_add ADD) const {\nreturn ProtocolXYZ_Share{ADD(i,b.i), ADD(ip1,b.ip1)};\n}\n</code></pre></p> <p>Primitives can utilize functions provided by the <code>Core</code> to invoke cryptographic operations and network communication. The following function might implement a multiplication protocol for our fictional protocol: <pre><code>template &lt;typename func_add, typename func_sub, typename func_mult&gt;\nProtocolXYZ_Share prepare_mult(ProtocolXYZ_Share b, func_add ADD, func_sub SUB, func_mult MULT) const {\nDatatype mask = getRandomVal(PNEXT);\nDatatype msg = ADD(MULT(i,b.i),mask);\nsend_to_live(PNEXT, msg);\nreturn ProtocolXYZ_Share{msg, mask};\n}\n\ntemplate &lt;typename func_add, typename func_sub&gt;\nvoid complete_mult(func_add ADD, func_sub SUB) {\nDatatype msg = recv_from_live(PPREV);\nip1 = SUB(ip1, msg)\n}\n</code></pre></p> <p>The following table provides an overview of the available operations.</p> Operation Description Example <code>send_to_live(PID,val)</code> Sends a message to a party in the online phase. <code>send_to_live(PNEXT, val)</code> -&gt; sends a message to the next party. <code>pre_send_to_live(PID,val)</code> Sends a message to a party in the preprocessing phase. <code>pre_send_to_live(P_1, val)</code> -&gt; sends a message to party 1 in the preprocessing phase. <code>recv_from_live(PID)</code> Receives a message from a party in the online phase. <code>recv_from_live(P_2)</code> -&gt; receives a message from party 2. <code>pre_recv_from_live(PID)</code> Receives a message from a party in the preprocessing phase. <code>pre_recv_from_live(PPREV)</code> -&gt; receives a message from the previous party in the preprocessing phase. <code>getRandomVal(PID)</code> Returns a random value. Can be invoked with different macros. For instance <code>getRandomVal(PSELF)</code> returns a random value from a seed held only by the party invoking the operation while <code>getRandomVal(PNEXT)</code> returns a shared random value with the seed held by both the party invoking the operation and the next party. <code>getRandomVal(P_012)</code> -&gt; random value held by P_0, P_1, P_2. <code>store_compare_view(PID, val)</code> Stores the value to compare its view with other parties at the end of the protocol. <code>store_compare_view(P_1, val)</code> -&gt; Compare the view of <code>val</code> with party 1 at the end of the protocol."},{"location":"protocols/#init-protocols","title":"Init Protocols","text":"<p>Init protocols only describe the communication pattern of the actual protocol. They require the same function signatures as the actual protocols. For instance, the earlier defined class might have an init protocol that looks like this: <pre><code>template &lt;typename Datatype&gt;\nclass ProtocolXYZ_Init{\nprivate:\n// no data required\npublic:\n\ntemplate &lt;typename func_add&gt;\nProtocolXYZ_Init Add(ProtocolXYZ_Init b, func_add ADD) const {\nreturn ProtocolXYZ_Init();\n}\n\ntemplate &lt;typename func_add, typename func_sub, typename func_mult&gt;\nProtocolXYZ_Init prepare_mult(ProtocolXYZ_Init b, func_add ADD, func_sub SUB, func_mult MULT) const {\nsend_to_(PNEXT); // corresponing init function for send_to_live\nreturn ProtocolXYZ_Init();\n}\n\ntemplate &lt;typename func_add, typename func_sub&gt;\nvoid complete_mult(func_add ADD, func_sub SUB) {\nreceive_from_(PPREV); // corresponing init function for recv_from_live\n}\n};\n</code></pre></p>"},{"location":"protocols/#post-protocols","title":"Post protocols","text":"<p>Some protocols offer a preprocessing phase. If the config option <code>pre=1</code> is set, the preprocessing phase is separated from the online phase. The online phase can then be implemented in the post-protocol. The post-protocol requires the same function signatures as the preprocessing protocol. Parties that only receive data in the preprocessing phase do not require a post-protocol. Instead, they automatically wait in the preprocessing phase until all data is received.</p> <p>Note</p> <p>If the config option <code>pre=0</code> is set, the post protocol is not executed and the offline and online phases are executed in the same protocol.</p> <p>Tip</p> <p>Only start implementing the post protocol after the actual protocol is implemented and tested with the option <code>pre=0</code>.</p>"}]}